{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab환경 및 파일 업로드",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdn71kFWufvPHphe1ZBwAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhdmsdud/TIL/blob/master/Colab%ED%99%98%EA%B2%BD_%EB%B0%8F_%ED%8C%8C%EC%9D%BC_%EC%97%85%EB%A1%9C%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIr246YEaEsz",
        "outputId": "165348fe-7b42-496a-8b70-c55ec0193a61"
      },
      "source": [
        "!cat /etc/issue.net"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ubuntu 18.04.5 LTS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URs2_zila2Rs",
        "outputId": "4cc2fd4d-da2e-4cd3-cf50-dcedfdd4304d"
      },
      "source": [
        "!head /proc/cpuinfo"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIYZoRm1a8av",
        "outputId": "af92e54d-08f5-41e2-ea53-df62d354b379"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Mar 29 05:19:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9xThhv9bDyT",
        "outputId": "7144eabc-9215-4f35-ff76-08c7bbf8338c"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHlnCmvtbgAZ"
      },
      "source": [
        "### upload file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmRQQzOFbSO2"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "sQGQ72wBbobr",
        "outputId": "1c7eb6b0-596b-4835-bf80-57d8d480f888"
      },
      "source": [
        "myfile = files.upload()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e5f4fbc2-2d4d-4e49-be0d-033f47a2dd76\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e5f4fbc2-2d4d-4e49-be0d-033f47a2dd76\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving auto-mpg.csv to auto-mpg.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yehOE9XubrzX",
        "outputId": "75401144-d260-4259-a126-8a73cc1f6f68"
      },
      "source": [
        "myfile"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auto-mpg.csv': b'\\xef\\xbb\\xbf18.0,8,307.0,130.0,3504.,12.0,70,1,\"chevrolet chevelle malibu\"\\r\\n15.0,8,350.0,165.0,3693.,11.5,70,1,\"buick skylark 320\"\\r\\n18.0,8,318.0,150.0,3436.,11.0,70,1,\"plymouth satellite\"\\r\\n16.0,8,304.0,150.0,3433.,12.0,70,1,\"amc rebel sst\"\\r\\n17.0,8,302.0,140.0,3449.,10.5,70,1,\"ford torino\"\\r\\n15.0,8,429.0,198.0,4341.,10.0,70,1,\"ford galaxie 500\"\\r\\n14.0,8,454.0,220.0,4354., 9.0,70,1,\"chevrolet impala\"\\r\\n14.0,8,440.0,215.0,4312., 8.5,70,1,\"plymouth fury iii\"\\r\\n14.0,8,455.0,225.0,4425.,10.0,70,1,\"pontiac catalina\"\\r\\n15.0,8,390.0,190.0,3850., 8.5,70,1,\"amc ambassador dpl\"\\r\\n15.0,8,383.0,170.0,3563.,10.0,70,1,\"dodge challenger se\"\\r\\n14.0,8,340.0,160.0,3609., 8.0,70,1,\"plymouth \\'cuda 340\"\\r\\n15.0,8,400.0,150.0,3761., 9.5,70,1,\"chevrolet monte carlo\"\\r\\n14.0,8,455.0,225.0,3086.,10.0,70,1,\"buick estate wagon (sw)\"\\r\\n24.0,4,113.0,95.00,2372.,15.0,70,3,\"toyota corona mark ii\"\\r\\n22.0,6,198.0,95.00,2833.,15.5,70,1,\"plymouth duster\"\\r\\n18.0,6,199.0,97.00,2774.,15.5,70,1,\"amc hornet\"\\r\\n21.0,6,200.0,85.00,2587.,16.0,70,1,\"ford maverick\"\\r\\n27.0,4,97.00,88.00,2130.,14.5,70,3,\"datsun pl510\"\\r\\n26.0,4,97.00,46.00,1835.,20.5,70,2,\"volkswagen 1131 deluxe sedan\"\\r\\n25.0,4,110.0,87.00,2672.,17.5,70,2,\"peugeot 504\"\\r\\n24.0,4,107.0,90.00,2430.,14.5,70,2,\"audi 100 ls\"\\r\\n25.0,4,104.0,95.00,2375.,17.5,70,2,\"saab 99e\"\\r\\n26.0,4,121.0,113.0,2234.,12.5,70,2,\"bmw 2002\"\\r\\n21.0,6,199.0,90.00,2648.,15.0,70,1,\"amc gremlin\"\\r\\n10.0,8,360.0,215.0,4615.,14.0,70,1,\"ford f250\"\\r\\n10.0,8,307.0,200.0,4376.,15.0,70,1,\"chevy c20\"\\r\\n11.0,8,318.0,210.0,4382.,13.5,70,1,\"dodge d200\"\\r\\n9.0, 8,304.0,193.0,4732.,18.5,70,1,\"hi 1200d\"\\r\\n27.0,4,97.00,88.00,2130.,14.5,71,3,\"datsun pl510\"\\r\\n28.0,4,140.0,90.00,2264.,15.5,71,1,\"chevrolet vega 2300\"\\r\\n25.0,4,113.0,95.00,2228.,14.0,71,3,\"toyota corona\"\\r\\n25.0,4,98.00,?, 2046.,19.0,71,1,\"ford pinto\"\\r\\n19.0,6,232.0,100.0,2634.,13.0,71,1,\"amc gremlin\"\\r\\n16.0,6,225.0,105.0,3439.,15.5,71,1,\"plymouth satellite custom\"\\r\\n17.0,6,250.0,100.0,3329.,15.5,71,1,\"chevrolet chevelle malibu\"\\r\\n19.0,6,250.0,88.00,3302.,15.5,71,1,\"ford torino 500\"\\r\\n18.0,6,232.0,100.0,3288.,15.5,71,1,\"amc matador\"\\r\\n14.0,8,350.0,165.0,4209.,12.0,71,1,\"chevrolet impala\"\\r\\n14.0,8,400.0,175.0,4464.,11.5,71,1,\"pontiac catalina brougham\"\\r\\n14.0,8,351.0,153.0,4154.,13.5,71,1,\"ford galaxie 500\"\\r\\n14.0,8,318.0,150.0,4096.,13.0,71,1,\"plymouth fury iii\"\\r\\n12.0,8,383.0,180.0,4955.,11.5,71,1,\"dodge monaco (sw)\"\\r\\n13.0,8,400.0,170.0,4746.,12.0,71,1,\"ford country squire (sw)\"\\r\\n13.0,8,400.0,175.0,5140.,12.0,71,1,\"pontiac safari (sw)\"\\r\\n18.0,6,258.0,110.0,2962.,13.5,71,1,\"amc hornet sportabout (sw)\"\\r\\n22.0,4,140.0,72.00,2408.,19.0,71,1,\"chevrolet vega (sw)\"\\r\\n19.0,6,250.0,100.0,3282.,15.0,71,1,\"pontiac firebird\"\\r\\n18.0,6,250.0,88.00,3139.,14.5,71,1,\"ford mustang\"\\r\\n23.0,4,122.0,86.00,2220.,14.0,71,1,\"mercury capri 2000\"\\r\\n28.0,4,116.0,90.00,2123.,14.0,71,2,\"opel 1900\"\\r\\n30.0,4,79.00,70.00,2074.,19.5,71,2,\"peugeot 304\"\\r\\n30.0,4,88.00,76.00,2065.,14.5,71,2,\"fiat 124b\"\\r\\n31.0,4,71.00,65.00,1773.,19.0,71,3,\"toyota corolla 1200\"\\r\\n35.0,4,72.00,69.00,1613.,18.0,71,3,\"datsun 1200\"\\r\\n27.0,4,97.00,60.00,1834.,19.0,71,2,\"volkswagen model 111\"\\r\\n26.0,4,91.00,70.00,1955.,20.5,71,1,\"plymouth cricket\"\\r\\n24.0,4,113.0,95.00,2278.,15.5,72,3,\"toyota corona hardtop\"\\r\\n25.0,4,97.50,80.00,2126.,17.0,72,1,\"dodge colt hardtop\"\\r\\n23.0,4,97.00,54.00,2254.,23.5,72,2,\"volkswagen type 3\"\\r\\n20.0,4,140.0,90.00,2408.,19.5,72,1,\"chevrolet vega\"\\r\\n21.0,4,122.0,86.00,2226.,16.5,72,1,\"ford pinto runabout\"\\r\\n13.0,8,350.0,165.0,4274.,12.0,72,1,\"chevrolet impala\"\\r\\n14.0,8,400.0,175.0,4385.,12.0,72,1,\"pontiac catalina\"\\r\\n15.0,8,318.0,150.0,4135.,13.5,72,1,\"plymouth fury iii\"\\r\\n14.0,8,351.0,153.0,4129.,13.0,72,1,\"ford galaxie 500\"\\r\\n17.0,8,304.0,150.0,3672.,11.5,72,1,\"amc ambassador sst\"\\r\\n11.0,8,429.0,208.0,4633.,11.0,72,1,\"mercury marquis\"\\r\\n13.0,8,350.0,155.0,4502.,13.5,72,1,\"buick lesabre custom\"\\r\\n12.0,8,350.0,160.0,4456.,13.5,72,1,\"oldsmobile delta 88 royale\"\\r\\n13.0,8,400.0,190.0,4422.,12.5,72,1,\"chrysler newport royal\"\\r\\n19.0,3,70.00,97.00,2330.,13.5,72,3,\"mazda rx2 coupe\"\\r\\n15.0,8,304.0,150.0,3892.,12.5,72,1,\"amc matador (sw)\"\\r\\n13.0,8,307.0,130.0,4098.,14.0,72,1,\"chevrolet chevelle concours (sw)\"\\r\\n13.0,8,302.0,140.0,4294.,16.0,72,1,\"ford gran torino (sw)\"\\r\\n14.0,8,318.0,150.0,4077.,14.0,72,1,\"plymouth satellite custom (sw)\"\\r\\n18.0,4,121.0,112.0,2933.,14.5,72,2,\"volvo 145e (sw)\"\\r\\n22.0,4,121.0,76.00,2511.,18.0,72,2,\"volkswagen 411 (sw)\"\\r\\n21.0,4,120.0,87.00,2979.,19.5,72,2,\"peugeot 504 (sw)\"\\r\\n26.0,4,96.00,69.00,2189.,18.0,72,2,\"renault 12 (sw)\"\\r\\n22.0,4,122.0,86.00,2395.,16.0,72,1,\"ford pinto (sw)\"\\r\\n28.0,4,97.00,92.00,2288.,17.0,72,3,\"datsun 510 (sw)\"\\r\\n23.0,4,120.0,97.00,2506.,14.5,72,3,\"toyouta corona mark ii (sw)\"\\r\\n28.0,4,98.00,80.00,2164.,15.0,72,1,\"dodge colt (sw)\"\\r\\n27.0,4,97.00,88.00,2100.,16.5,72,3,\"toyota corolla 1600 (sw)\"\\r\\n13.0,8,350.0,175.0,4100.,13.0,73,1,\"buick century 350\"\\r\\n14.0,8,304.0,150.0,3672.,11.5,73,1,\"amc matador\"\\r\\n13.0,8,350.0,145.0,3988.,13.0,73,1,\"chevrolet malibu\"\\r\\n14.0,8,302.0,137.0,4042.,14.5,73,1,\"ford gran torino\"\\r\\n15.0,8,318.0,150.0,3777.,12.5,73,1,\"dodge coronet custom\"\\r\\n12.0,8,429.0,198.0,4952.,11.5,73,1,\"mercury marquis brougham\"\\r\\n13.0,8,400.0,150.0,4464.,12.0,73,1,\"chevrolet caprice classic\"\\r\\n13.0,8,351.0,158.0,4363.,13.0,73,1,\"ford ltd\"\\r\\n14.0,8,318.0,150.0,4237.,14.5,73,1,\"plymouth fury gran sedan\"\\r\\n13.0,8,440.0,215.0,4735.,11.0,73,1,\"chrysler new yorker brougham\"\\r\\n12.0,8,455.0,225.0,4951.,11.0,73,1,\"buick electra 225 custom\"\\r\\n13.0,8,360.0,175.0,3821.,11.0,73,1,\"amc ambassador brougham\"\\r\\n18.0,6,225.0,105.0,3121.,16.5,73,1,\"plymouth valiant\"\\r\\n16.0,6,250.0,100.0,3278.,18.0,73,1,\"chevrolet nova custom\"\\r\\n18.0,6,232.0,100.0,2945.,16.0,73,1,\"amc hornet\"\\r\\n18.0,6,250.0,88.00,3021.,16.5,73,1,\"ford maverick\"\\r\\n23.0,6,198.0,95.00,2904.,16.0,73,1,\"plymouth duster\"\\r\\n26.0,4,97.00,46.00,1950.,21.0,73,2,\"volkswagen super beetle\"\\r\\n11.0,8,400.0,150.0,4997.,14.0,73,1,\"chevrolet impala\"\\r\\n12.0,8,400.0,167.0,4906.,12.5,73,1,\"ford country\"\\r\\n13.0,8,360.0,170.0,4654.,13.0,73,1,\"plymouth custom suburb\"\\r\\n12.0,8,350.0,180.0,4499.,12.5,73,1,\"oldsmobile vista cruiser\"\\r\\n18.0,6,232.0,100.0,2789.,15.0,73,1,\"amc gremlin\"\\r\\n20.0,4,97.00,88.00,2279.,19.0,73,3,\"toyota carina\"\\r\\n21.0,4,140.0,72.00,2401.,19.5,73,1,\"chevrolet vega\"\\r\\n22.0,4,108.0,94.00,2379.,16.5,73,3,\"datsun 610\"\\r\\n18.0,3,70.00,90.00,2124.,13.5,73,3,\"maxda rx3\"\\r\\n19.0,4,122.0,85.00,2310.,18.5,73,1,\"ford pinto\"\\r\\n21.0,6,155.0,107.0,2472.,14.0,73,1,\"mercury capri v6\"\\r\\n26.0,4,98.00,90.00,2265.,15.5,73,2,\"fiat 124 sport coupe\"\\r\\n15.0,8,350.0,145.0,4082.,13.0,73,1,\"chevrolet monte carlo s\"\\r\\n16.0,8,400.0,230.0,4278.,9.50,73,1,\"pontiac grand prix\"\\r\\n29.0,4,68.00,49.00,1867.,19.5,73,2,\"fiat 128\"\\r\\n24.0,4,116.0,75.00,2158.,15.5,73,2,\"opel manta\"\\r\\n20.0,4,114.0,91.00,2582.,14.0,73,2,\"audi 100ls\"\\r\\n19.0,4,121.0,112.0,2868.,15.5,73,2,\"volvo 144ea\"\\r\\n15.0,8,318.0,150.0,3399.,11.0,73,1,\"dodge dart custom\"\\r\\n24.0,4,121.0,110.0,2660.,14.0,73,2,\"saab 99le\"\\r\\n20.0,6,156.0,122.0,2807.,13.5,73,3,\"toyota mark ii\"\\r\\n11.0,8,350.0,180.0,3664.,11.0,73,1,\"oldsmobile omega\"\\r\\n20.0,6,198.0,95.00,3102.,16.5,74,1,\"plymouth duster\"\\r\\n21.0,6,200.0,?, 2875.,17.0,74,1,\"ford maverick\"\\r\\n19.0,6,232.0,100.0,2901.,16.0,74,1,\"amc hornet\"\\r\\n15.0,6,250.0,100.0,3336.,17.0,74,1,\"chevrolet nova\"\\r\\n31.0,4,79.00,67.00,1950.,19.0,74,3,\"datsun b210\"\\r\\n26.0,4,122.0,80.00,2451.,16.5,74,1,\"ford pinto\"\\r\\n32.0,4,71.00,65.00,1836.,21.0,74,3,\"toyota corolla 1200\"\\r\\n25.0,4,140.0,75.00,2542.,17.0,74,1,\"chevrolet vega\"\\r\\n16.0,6,250.0,100.0,3781.,17.0,74,1,\"chevrolet chevelle malibu classic\"\\r\\n16.0,6,258.0,110.0,3632.,18.0,74,1,\"amc matador\"\\r\\n18.0,6,225.0,105.0,3613.,16.5,74,1,\"plymouth satellite sebring\"\\r\\n16.0,8,302.0,140.0,4141.,14.0,74,1,\"ford gran torino\"\\r\\n13.0,8,350.0,150.0,4699.,14.5,74,1,\"buick century luxus (sw)\"\\r\\n14.0,8,318.0,150.0,4457.,13.5,74,1,\"dodge coronet custom (sw)\"\\r\\n14.0,8,302.0,140.0,4638.,16.0,74,1,\"ford gran torino (sw)\"\\r\\n14.0,8,304.0,150.0,4257.,15.5,74,1,\"amc matador (sw)\"\\r\\n29.0,4,98.00,83.00,2219.,16.5,74,2,\"audi fox\"\\r\\n26.0,4,79.00,67.00,1963.,15.5,74,2,\"volkswagen dasher\"\\r\\n26.0,4,97.00,78.00,2300.,14.5,74,2,\"opel manta\"\\r\\n31.0,4,76.00,52.00,1649.,16.5,74,3,\"toyota corona\"\\r\\n32.0,4,83.00,61.00,2003.,19.0,74,3,\"datsun 710\"\\r\\n28.0,4,90.00,75.00,2125.,14.5,74,1,\"dodge colt\"\\r\\n24.0,4,90.00,75.00,2108.,15.5,74,2,\"fiat 128\"\\r\\n26.0,4,116.0,75.00,2246.,14.0,74,2,\"fiat 124 tc\"\\r\\n24.0,4,120.0,97.00,2489.,15.0,74,3,\"honda civic\"\\r\\n26.0,4,108.0,93.00,2391.,15.5,74,3,\"subaru\"\\r\\n31.0,4,79.00,67.00,2000.,16.0,74,2,\"fiat x1.9\"\\r\\n19.0,6,225.0,95.00,3264.,16.0,75,1,\"plymouth valiant custom\"\\r\\n18.0,6,250.0,105.0,3459.,16.0,75,1,\"chevrolet nova\"\\r\\n15.0,6,250.0,72.00,3432.,21.0,75,1,\"mercury monarch\"\\r\\n15.0,6,250.0,72.00,3158.,19.5,75,1,\"ford maverick\"\\r\\n16.0,8,400.0,170.0,4668.,11.5,75,1,\"pontiac catalina\"\\r\\n15.0,8,350.0,145.0,4440.,14.0,75,1,\"chevrolet bel air\"\\r\\n16.0,8,318.0,150.0,4498.,14.5,75,1,\"plymouth grand fury\"\\r\\n14.0,8,351.0,148.0,4657.,13.5,75,1,\"ford ltd\"\\r\\n17.0,6,231.0,110.0,3907.,21.0,75,1,\"buick century\"\\r\\n16.0,6,250.0,105.0,3897.,18.5,75,1,\"chevroelt chevelle malibu\"\\r\\n15.0,6,258.0,110.0,3730.,19.0,75,1,\"amc matador\"\\r\\n18.0,6,225.0,95.00,3785.,19.0,75,1,\"plymouth fury\"\\r\\n21.0,6,231.0,110.0,3039.,15.0,75,1,\"buick skyhawk\"\\r\\n20.0,8,262.0,110.0,3221.,13.5,75,1,\"chevrolet monza 2+2\"\\r\\n13.0,8,302.0,129.0,3169.,12.0,75,1,\"ford mustang ii\"\\r\\n29.0,4,97.00,75.00,2171.,16.0,75,3,\"toyota corolla\"\\r\\n23.0,4,140.0,83.00,2639.,17.0,75,1,\"ford pinto\"\\r\\n20.0,6,232.0,100.0,2914.,16.0,75,1,\"amc gremlin\"\\r\\n23.0,4,140.0,78.00,2592.,18.5,75,1,\"pontiac astro\"\\r\\n24.0,4,134.0,96.00,2702.,13.5,75,3,\"toyota corona\"\\r\\n25.0,4,90.00,71.00,2223.,16.5,75,2,\"volkswagen dasher\"\\r\\n24.0,4,119.0,97.00,2545.,17.0,75,3,\"datsun 710\"\\r\\n18.0,6,171.0,97.00,2984.,14.5,75,1,\"ford pinto\"\\r\\n29.0,4,90.00,70.00,1937.,14.0,75,2,\"volkswagen rabbit\"\\r\\n19.0,6,232.0,90.00,3211.,17.0,75,1,\"amc pacer\"\\r\\n23.0,4,115.0,95.00,2694.,15.0,75,2,\"audi 100ls\"\\r\\n23.0,4,120.0,88.00,2957.,17.0,75,2,\"peugeot 504\"\\r\\n22.0,4,121.0,98.00,2945.,14.5,75,2,\"volvo 244dl\"\\r\\n25.0,4,121.0,115.0,2671.,13.5,75,2,\"saab 99le\"\\r\\n33.0,4,91.00,53.00,1795.,17.5,75,3,\"honda civic cvcc\"\\r\\n28.0,4,107.0,86.00,2464.,15.5,76,2,\"fiat 131\"\\r\\n25.0,4,116.0,81.00,2220.,16.9,76,2,\"opel 1900\"\\r\\n25.0,4,140.0,92.00,2572.,14.9,76,1,\"capri ii\"\\r\\n26.0,4,98.00,79.00,2255.,17.7,76,1,\"dodge colt\"\\r\\n27.0,4,101.0,83.00,2202.,15.3,76,2,\"renault 12tl\"\\r\\n17.5,8,305.0,140.0,4215.,13.0,76,1,\"chevrolet chevelle malibu classic\"\\r\\n16.0,8,318.0,150.0,4190.,13.0,76,1,\"dodge coronet brougham\"\\r\\n15.5,8,304.0,120.0,3962.,13.9,76,1,\"amc matador\"\\r\\n14.5,8,351.0,152.0,4215.,12.8,76,1,\"ford gran torino\"\\r\\n22.0,6,225.0,100.0,3233.,15.4,76,1,\"plymouth valiant\"\\r\\n22.0,6,250.0,105.0,3353.,14.5,76,1,\"chevrolet nova\"\\r\\n24.0,6,200.0,81.00,3012.,17.6,76,1,\"ford maverick\"\\r\\n22.5,6,232.0,90.00,3085.,17.6,76,1,\"amc hornet\"\\r\\n29.0,4,85.00,52.00,2035.,22.2,76,1,\"chevrolet chevette\"\\r\\n24.5,4,98.00,60.00,2164.,22.1,76,1,\"chevrolet woody\"\\r\\n29.0,4,90.00,70.00,1937.,14.2,76,2,\"vw rabbit\"\\r\\n33.0,4,91.00,53.00,1795.,17.4,76,3,\"honda civic\"\\r\\n20.0,6,225.0,100.0,3651.,17.7,76,1,\"dodge aspen se\"\\r\\n18.0,6,250.0,78.00,3574.,21.0,76,1,\"ford granada ghia\"\\r\\n18.5,6,250.0,110.0,3645.,16.2,76,1,\"pontiac ventura sj\"\\r\\n17.5,6,258.0,95.00,3193.,17.8,76,1,\"amc pacer d/l\"\\r\\n29.5,4,97.00,71.00,1825.,12.2,76,2,\"volkswagen rabbit\"\\r\\n32.0,4,85.00,70.00,1990.,17.0,76,3,\"datsun b-210\"\\r\\n28.0,4,97.00,75.00,2155.,16.4,76,3,\"toyota corolla\"\\r\\n26.5,4,140.0,72.00,2565.,13.6,76,1,\"ford pinto\"\\r\\n20.0,4,130.0,102.0,3150.,15.7,76,2,\"volvo 245\"\\r\\n13.0,8,318.0,150.0,3940.,13.2,76,1,\"plymouth volare premier v8\"\\r\\n19.0,4,120.0,88.00,3270.,21.9,76,2,\"peugeot 504\"\\r\\n19.0,6,156.0,108.0,2930.,15.5,76,3,\"toyota mark ii\"\\r\\n16.5,6,168.0,120.0,3820.,16.7,76,2,\"mercedes-benz 280s\"\\r\\n16.5,8,350.0,180.0,4380.,12.1,76,1,\"cadillac seville\"\\r\\n13.0,8,350.0,145.0,4055.,12.0,76,1,\"chevy c10\"\\r\\n13.0,8,302.0,130.0,3870.,15.0,76,1,\"ford f108\"\\r\\n13.0,8,318.0,150.0,3755.,14.0,76,1,\"dodge d100\"\\r\\n31.5,4,98.00,68.00,2045.,18.5,77,3,\"honda accord cvcc\"\\r\\n30.0,4,111.0,80.00,2155.,14.8,77,1,\"buick opel isuzu deluxe\"\\r\\n36.0,4,79.00,58.00,1825.,18.6,77,2,\"renault 5 gtl\"\\r\\n25.5,4,122.0,96.00,2300.,15.5,77,1,\"plymouth arrow gs\"\\r\\n33.5,4,85.00,70.00,1945.,16.8,77,3,\"datsun f-10 hatchback\"\\r\\n17.5,8,305.0,145.0,3880.,12.5,77,1,\"chevrolet caprice classic\"\\r\\n17.0,8,260.0,110.0,4060.,19.0,77,1,\"oldsmobile cutlass supreme\"\\r\\n15.5,8,318.0,145.0,4140.,13.7,77,1,\"dodge monaco brougham\"\\r\\n15.0,8,302.0,130.0,4295.,14.9,77,1,\"mercury cougar brougham\"\\r\\n17.5,6,250.0,110.0,3520.,16.4,77,1,\"chevrolet concours\"\\r\\n20.5,6,231.0,105.0,3425.,16.9,77,1,\"buick skylark\"\\r\\n19.0,6,225.0,100.0,3630.,17.7,77,1,\"plymouth volare custom\"\\r\\n18.5,6,250.0,98.00,3525.,19.0,77,1,\"ford granada\"\\r\\n16.0,8,400.0,180.0,4220.,11.1,77,1,\"pontiac grand prix lj\"\\r\\n15.5,8,350.0,170.0,4165.,11.4,77,1,\"chevrolet monte carlo landau\"\\r\\n15.5,8,400.0,190.0,4325.,12.2,77,1,\"chrysler cordoba\"\\r\\n16.0,8,351.0,149.0,4335.,14.5,77,1,\"ford thunderbird\"\\r\\n29.0,4,97.00,78.00,1940.,14.5,77,2,\"volkswagen rabbit custom\"\\r\\n24.5,4,151.0,88.00,2740.,16.0,77,1,\"pontiac sunbird coupe\"\\r\\n26.0,4,97.00,75.00,2265.,18.2,77,3,\"toyota corolla liftback\"\\r\\n25.5,4,140.0,89.00,2755.,15.8,77,1,\"ford mustang ii 2+2\"\\r\\n30.5,4,98.00,63.00,2051.,17.0,77,1,\"chevrolet chevette\"\\r\\n33.5,4,98.00,83.00,2075.,15.9,77,1,\"dodge colt m/m\"\\r\\n30.0,4,97.00,67.00,1985.,16.4,77,3,\"subaru dl\"\\r\\n30.5,4,97.00,78.00,2190.,14.1,77,2,\"volkswagen dasher\"\\r\\n22.0,6,146.0,97.00,2815.,14.5,77,3,\"datsun 810\"\\r\\n21.5,4,121.0,110.0,2600.,12.8,77,2,\"bmw 320i\"\\r\\n21.5,3,80.00,110.0,2720.,13.5,77,3,\"mazda rx-4\"\\r\\n43.1,4,90.00,48.00,1985.,21.5,78,2,\"volkswagen rabbit custom diesel\"\\r\\n36.1,4,98.00,66.00,1800.,14.4,78,1,\"ford fiesta\"\\r\\n32.8,4,78.00,52.00,1985.,19.4,78,3,\"mazda glc deluxe\"\\r\\n39.4,4,85.00,70.00,2070.,18.6,78,3,\"datsun b210 gx\"\\r\\n36.1,4,91.00,60.00,1800.,16.4,78,3,\"honda civic cvcc\"\\r\\n19.9,8,260.0,110.0,3365.,15.5,78,1,\"oldsmobile cutlass salon brougham\"\\r\\n19.4,8,318.0,140.0,3735.,13.2,78,1,\"dodge diplomat\"\\r\\n20.2,8,302.0,139.0,3570.,12.8,78,1,\"mercury monarch ghia\"\\r\\n19.2,6,231.0,105.0,3535.,19.2,78,1,\"pontiac phoenix lj\"\\r\\n20.5,6,200.0,95.00,3155.,18.2,78,1,\"chevrolet malibu\"\\r\\n20.2,6,200.0,85.00,2965.,15.8,78,1,\"ford fairmont (auto)\"\\r\\n25.1,4,140.0,88.00,2720.,15.4,78,1,\"ford fairmont (man)\"\\r\\n20.5,6,225.0,100.0,3430.,17.2,78,1,\"plymouth volare\"\\r\\n19.4,6,232.0,90.00,3210.,17.2,78,1,\"amc concord\"\\r\\n20.6,6,231.0,105.0,3380.,15.8,78,1,\"buick century special\"\\r\\n20.8,6,200.0,85.00,3070.,16.7,78,1,\"mercury zephyr\"\\r\\n18.6,6,225.0,110.0,3620.,18.7,78,1,\"dodge aspen\"\\r\\n18.1,6,258.0,120.0,3410.,15.1,78,1,\"amc concord d/l\"\\r\\n19.2,8,305.0,145.0,3425.,13.2,78,1,\"chevrolet monte carlo landau\"\\r\\n17.7,6,231.0,165.0,3445.,13.4,78,1,\"buick regal sport coupe (turbo)\"\\r\\n18.1,8,302.0,139.0,3205.,11.2,78,1,\"ford futura\"\\r\\n17.5,8,318.0,140.0,4080.,13.7,78,1,\"dodge magnum xe\"\\r\\n30.0,4,98.00,68.00,2155.,16.5,78,1,\"chevrolet chevette\"\\r\\n27.5,4,134.0,95.00,2560.,14.2,78,3,\"toyota corona\"\\r\\n27.2,4,119.0,97.00,2300.,14.7,78,3,\"datsun 510\"\\r\\n30.9,4,105.0,75.00,2230.,14.5,78,1,\"dodge omni\"\\r\\n21.1,4,134.0,95.00,2515.,14.8,78,3,\"toyota celica gt liftback\"\\r\\n23.2,4,156.0,105.0,2745.,16.7,78,1,\"plymouth sapporo\"\\r\\n23.8,4,151.0,85.00,2855.,17.6,78,1,\"oldsmobile starfire sx\"\\r\\n23.9,4,119.0,97.00,2405.,14.9,78,3,\"datsun 200-sx\"\\r\\n20.3,5,131.0,103.0,2830.,15.9,78,2,\"audi 5000\"\\r\\n17.0,6,163.0,125.0,3140.,13.6,78,2,\"volvo 264gl\"\\r\\n21.6,4,121.0,115.0,2795.,15.7,78,2,\"saab 99gle\"\\r\\n16.2,6,163.0,133.0,3410.,15.8,78,2,\"peugeot 604sl\"\\r\\n31.5,4,89.00,71.00,1990.,14.9,78,2,\"volkswagen scirocco\"\\r\\n29.5,4,98.00,68.00,2135.,16.6,78,3,\"honda accord lx\"\\r\\n21.5,6,231.0,115.0,3245.,15.4,79,1,\"pontiac lemans v6\"\\r\\n19.8,6,200.0,85.00,2990.,18.2,79,1,\"mercury zephyr 6\"\\r\\n22.3,4,140.0,88.00,2890.,17.3,79,1,\"ford fairmont 4\"\\r\\n20.2,6,232.0,90.00,3265.,18.2,79,1,\"amc concord dl 6\"\\r\\n20.6,6,225.0,110.0,3360.,16.6,79,1,\"dodge aspen 6\"\\r\\n17.0,8,305.0,130.0,3840.,15.4,79,1,\"chevrolet caprice classic\"\\r\\n17.6,8,302.0,129.0,3725.,13.4,79,1,\"ford ltd landau\"\\r\\n16.5,8,351.0,138.0,3955.,13.2,79,1,\"mercury grand marquis\"\\r\\n18.2,8,318.0,135.0,3830.,15.2,79,1,\"dodge st. regis\"\\r\\n16.9,8,350.0,155.0,4360.,14.9,79,1,\"buick estate wagon (sw)\"\\r\\n15.5,8,351.0,142.0,4054.,14.3,79,1,\"ford country squire (sw)\"\\r\\n19.2,8,267.0,125.0,3605.,15.0,79,1,\"chevrolet malibu classic (sw)\"\\r\\n18.5,8,360.0,150.0,3940.,13.0,79,1,\"chrysler lebaron town @ country (sw)\"\\r\\n31.9,4,89.00,71.00,1925.,14.0,79,2,\"vw rabbit custom\"\\r\\n34.1,4,86.00,65.00,1975.,15.2,79,3,\"maxda glc deluxe\"\\r\\n35.7,4,98.00,80.00,1915.,14.4,79,1,\"dodge colt hatchback custom\"\\r\\n27.4,4,121.0,80.00,2670.,15.0,79,1,\"amc spirit dl\"\\r\\n25.4,5,183.0,77.00,3530.,20.1,79,2,\"mercedes benz 300d\"\\r\\n23.0,8,350.0,125.0,3900.,17.4,79,1,\"cadillac eldorado\"\\r\\n27.2,4,141.0,71.00,3190.,24.8,79,2,\"peugeot 504\"\\r\\n23.9,8,260.0,90.00,3420.,22.2,79,1,\"oldsmobile cutlass salon brougham\"\\r\\n34.2,4,105.0,70.00,2200.,13.2,79,1,\"plymouth horizon\"\\r\\n34.5,4,105.0,70.00,2150.,14.9,79,1,\"plymouth horizon tc3\"\\r\\n31.8,4,85.00,65.00,2020.,19.2,79,3,\"datsun 210\"\\r\\n37.3,4,91.00,69.00,2130.,14.7,79,2,\"fiat strada custom\"\\r\\n28.4,4,151.0,90.00,2670.,16.0,79,1,\"buick skylark limited\"\\r\\n28.8,6,173.0,115.0,2595.,11.3,79,1,\"chevrolet citation\"\\r\\n26.8,6,173.0,115.0,2700.,12.9,79,1,\"oldsmobile omega brougham\"\\r\\n33.5,4,151.0,90.00,2556.,13.2,79,1,\"pontiac phoenix\"\\r\\n41.5,4,98.00,76.00,2144.,14.7,80,2,\"vw rabbit\"\\r\\n38.1,4,89.00,60.00,1968.,18.8,80,3,\"toyota corolla tercel\"\\r\\n32.1,4,98.00,70.00,2120.,15.5,80,1,\"chevrolet chevette\"\\r\\n37.2,4,86.00,65.00,2019.,16.4,80,3,\"datsun 310\"\\r\\n28.0,4,151.0,90.00,2678.,16.5,80,1,\"chevrolet citation\"\\r\\n26.4,4,140.0,88.00,2870.,18.1,80,1,\"ford fairmont\"\\r\\n24.3,4,151.0,90.00,3003.,20.1,80,1,\"amc concord\"\\r\\n19.1,6,225.0,90.00,3381.,18.7,80,1,\"dodge aspen\"\\r\\n34.3,4,97.00,78.00,2188.,15.8,80,2,\"audi 4000\"\\r\\n29.8,4,134.0,90.00,2711.,15.5,80,3,\"toyota corona liftback\"\\r\\n31.3,4,120.0,75.00,2542.,17.5,80,3,\"mazda 626\"\\r\\n37.0,4,119.0,92.00,2434.,15.0,80,3,\"datsun 510 hatchback\"\\r\\n32.2,4,108.0,75.00,2265.,15.2,80,3,\"toyota corolla\"\\r\\n46.6,4,86.00,65.00,2110.,17.9,80,3,\"mazda glc\"\\r\\n27.9,4,156.0,105.0,2800.,14.4,80,1,\"dodge colt\"\\r\\n40.8,4,85.00,65.00,2110.,19.2,80,3,\"datsun 210\"\\r\\n44.3,4,90.00,48.00,2085.,21.7,80,2,\"vw rabbit c (diesel)\"\\r\\n43.4,4,90.00,48.00,2335.,23.7,80,2,\"vw dasher (diesel)\"\\r\\n36.4,5,121.0,67.00,2950.,19.9,80,2,\"audi 5000s (diesel)\"\\r\\n30.0,4,146.0,67.00,3250.,21.8,80,2,\"mercedes-benz 240d\"\\r\\n44.6,4,91.00,67.00,1850.,13.8,80,3,\"honda civic 1500 gl\"\\r\\n40.9,4,85.00,?, 1835.,17.3,80,2,\"renault lecar deluxe\"\\r\\n33.8,4,97.00,67.00,2145.,18.0,80,3,\"subaru dl\"\\r\\n29.8,4,89.00,62.00,1845.,15.3,80,2,\"vokswagen rabbit\"\\r\\n32.7,6,168.0,132.0,2910.,11.4,80,3,\"datsun 280-zx\"\\r\\n23.7,3,70.00,100.0,2420.,12.5,80,3,\"mazda rx-7 gs\"\\r\\n35.0,4,122.0,88.00,2500.,15.1,80,2,\"triumph tr7 coupe\"\\r\\n23.6,4,140.0,?, 2905.,14.3,80,1,\"ford mustang cobra\"\\r\\n32.4,4,107.0,72.00,2290.,17.0,80,3,\"honda accord\"\\r\\n27.2,4,135.0,84.00,2490.,15.7,81,1,\"plymouth reliant\"\\r\\n26.6,4,151.0,84.00,2635.,16.4,81,1,\"buick skylark\"\\r\\n25.8,4,156.0,92.00,2620.,14.4,81,1,\"dodge aries wagon (sw)\"\\r\\n23.5,6,173.0,110.0,2725.,12.6,81,1,\"chevrolet citation\"\\r\\n30.0,4,135.0,84.00,2385.,12.9,81,1,\"plymouth reliant\"\\r\\n39.1,4,79.00,58.00,1755.,16.9,81,3,\"toyota starlet\"\\r\\n39.0,4,86.00,64.00,1875.,16.4,81,1,\"plymouth champ\"\\r\\n35.1,4,81.00,60.00,1760.,16.1,81,3,\"honda civic 1300\"\\r\\n32.3,4,97.00,67.00,2065.,17.8,81,3,\"subaru\"\\r\\n37.0,4,85.00,65.00,1975.,19.4,81,3,\"datsun 210 mpg\"\\r\\n37.7,4,89.00,62.00,2050.,17.3,81,3,\"toyota tercel\"\\r\\n34.1,4,91.00,68.00,1985.,16.0,81,3,\"mazda glc 4\"\\r\\n34.7,4,105.0,63.00,2215.,14.9,81,1,\"plymouth horizon 4\"\\r\\n34.4,4,98.00,65.00,2045.,16.2,81,1,\"ford escort 4w\"\\r\\n29.9,4,98.00,65.00,2380.,20.7,81,1,\"ford escort 2h\"\\r\\n33.0,4,105.0,74.00,2190.,14.2,81,2,\"volkswagen jetta\"\\r\\n34.5,4,100.0,?, 2320.,15.8,81,2,\"renault 18i\"\\r\\n33.7,4,107.0,75.00,2210.,14.4,81,3,\"honda prelude\"\\r\\n32.4,4,108.0,75.00,2350.,16.8,81,3,\"toyota corolla\"\\r\\n32.9,4,119.0,100.0,2615.,14.8,81,3,\"datsun 200sx\"\\r\\n31.6,4,120.0,74.00,2635.,18.3,81,3,\"mazda 626\"\\r\\n28.1,4,141.0,80.00,3230.,20.4,81,2,\"peugeot 505s turbo diesel\"\\r\\n30.7,6,145.0,76.00,3160.,19.6,81,2,\"volvo diesel\"\\r\\n25.4,6,168.0,116.0,2900.,12.6,81,3,\"toyota cressida\"\\r\\n24.2,6,146.0,120.0,2930.,13.8,81,3,\"datsun 810 maxima\"\\r\\n22.4,6,231.0,110.0,3415.,15.8,81,1,\"buick century\"\\r\\n26.6,8,350.0,105.0,3725.,19.0,81,1,\"oldsmobile cutlass ls\"\\r\\n20.2,6,200.0,88.00,3060.,17.1,81,1,\"ford granada gl\"\\r\\n17.6,6,225.0,85.00,3465.,16.6,81,1,\"chrysler lebaron salon\"\\r\\n28.0,4,112.0,88.00,2605.,19.6,82,1,\"chevrolet cavalier\"\\r\\n27.0,4,112.0,88.00,2640.,18.6,82,1,\"chevrolet cavalier wagon\"\\r\\n34.0,4,112.0,88.00,2395.,18.0,82,1,\"chevrolet cavalier 2-door\"\\r\\n31.0,4,112.0,85.00,2575.,16.2,82,1,\"pontiac j2000 se hatchback\"\\r\\n29.0,4,135.0,84.00,2525.,16.0,82,1,\"dodge aries se\"\\r\\n27.0,4,151.0,90.00,2735.,18.0,82,1,\"pontiac phoenix\"\\r\\n24.0,4,140.0,92.00,2865.,16.4,82,1,\"ford fairmont futura\"\\r\\n23.0,4,151.0,?, 3035.,20.5,82,1,\"amc concord dl\"\\r\\n36.0,4,105.0,74.00,1980.,15.3,82,2,\"volkswagen rabbit l\"\\r\\n37.0,4,91.00,68.00,2025.,18.2,82,3,\"mazda glc custom l\"\\r\\n31.0,4,91.00,68.00,1970.,17.6,82,3,\"mazda glc custom\"\\r\\n38.0,4,105.0,63.00,2125.,14.7,82,1,\"plymouth horizon miser\"\\r\\n36.0,4,98.00,70.00,2125.,17.3,82,1,\"mercury lynx l\"\\r\\n36.0,4,120.0,88.00,2160.,14.5,82,3,\"nissan stanza xe\"\\r\\n36.0,4,107.0,75.00,2205.,14.5,82,3,\"honda accord\"\\r\\n34.0,4,108.0,70.00,2245, 16.9,82,3,\"toyota corolla\"\\r\\n38.0,4,91.00,67.00,1965.,15.0,82,3,\"honda civic\"\\r\\n32.0,4,91.00,67.00,1965.,15.7,82,3,\"honda civic (auto)\"\\r\\n38.0,4,91.00,67.00,1995.,16.2,82,3,\"datsun 310 gx\"\\r\\n25.0,6,181.0,110.0,2945.,16.4,82,1,\"buick century limited\"\\r\\n38.0,6,262.0,85.00,3015.,17.0,82,1,\"oldsmobile cutlass ciera (diesel)\"\\r\\n26.0,4,156.0,92.00,2585.,14.5,82,1,\"chrysler lebaron medallion\"\\r\\n22.0,6,232.0,112.0,2835, 14.7,82,1,\"ford granada l\"\\r\\n32.0,4,144.0,96.00,2665.,13.9,82,3,\"toyota celica gt\"\\r\\n36.0,4,135.0,84.00,2370.,13.0,82,1,\"dodge charger 2.2\"\\r\\n27.0,4,151.0,90.00,2950.,17.3,82,1,\"chevrolet camaro\"\\r\\n27.0,4,140.0,86.00,2790.,15.6,82,1,\"ford mustang gl\"\\r\\n44.0,4,97.00,52.00,2130.,24.6,82,2,\"vw pickup\"\\r\\n32.0,4,135.0,84.00,2295.,11.6,82,1,\"dodge rampage\"\\r\\n28.0,4,120.0,79.00,2625.,18.6,82,1,\"ford ranger\"\\r\\n31.0,4,119.0,82.00,2720.,19.4,82,1,\"chevy s-10\"\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETKu200Eb0-d"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "1bevxgq6b5lK",
        "outputId": "7e85e11a-8fb0-4240-b767-ad9ff617ac85"
      },
      "source": [
        "datasets = pd.read_csv('auto-mpg.csv')\n",
        "datasets.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>18.0</th>\n",
              "      <th>8</th>\n",
              "      <th>307.0</th>\n",
              "      <th>130.0</th>\n",
              "      <th>3504.</th>\n",
              "      <th>12.0</th>\n",
              "      <th>70</th>\n",
              "      <th>1</th>\n",
              "      <th>chevrolet chevelle malibu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>3693.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>buick skylark 320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3436.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>plymouth satellite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3433.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>amc rebel sst</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>3449.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>ford torino</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>429.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>4341.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>ford galaxie 500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   18.0  8  307.0  130.0   3504.  12.0  70  1 chevrolet chevelle malibu\n",
              "0  15.0  8  350.0  165.0  3693.0  11.5  70  1         buick skylark 320\n",
              "1  18.0  8  318.0  150.0  3436.0  11.0  70  1        plymouth satellite\n",
              "2  16.0  8  304.0  150.0  3433.0  12.0  70  1             amc rebel sst\n",
              "3  17.0  8  302.0  140.0  3449.0  10.5  70  1               ford torino\n",
              "4  15.0  8  429.0  198.0  4341.0  10.0  70  1          ford galaxie 500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2bwE3DLb9cd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdBgnk_EcVN0"
      },
      "source": [
        "### jupyter --> colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_1ZbWIUlgsP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FluSYdT1cadA"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11bz9BPTcg92"
      },
      "source": [
        "(x_train, y_train) , (x_test , y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFwY70Nkcql5"
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train , 10)\n",
        "y_test = keras.utils.to_categorical(y_test , 10)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rimBmcpRcwIM"
      },
      "source": [
        "x_train = x_train.reshape(60000 , 784)\n",
        "x_test  = x_test.reshape(10000 , 784)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxZLmaqQcyVm"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train , x_val , y_train , y_val = train_test_split(x_train, y_train , \n",
        "                                                     test_size = 0.3 ,\n",
        "                                                     random_state = 100)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoUplyT-c08g"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Sequential , Model\n",
        "\n",
        "input   = Input(shape=(784, ) , name='input')\n",
        "\n",
        "hidden1 = Dense(256  , activation='sigmoid' , name='hidden1' )(input)\n",
        "hidden2 = Dense(128  , activation='sigmoid' , name='hidden2' )(hidden1)\n",
        "hidden3 = Dense(64  ,  activation='sigmoid' , name='hidden3' )(hidden2)\n",
        "hidden4 = Dense(32  ,  activation='sigmoid' , name='hidden4' )(hidden3)\n",
        "\n",
        "output  = Dense(10  ,  activation='softmax' , name='output' )(hidden4)\n",
        "\n",
        "model   = Model(inputs=[input] , outputs=[output]) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcIahgzTc8t4"
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "model.compile(optimizer = optimizer, \n",
        "              loss      = 'categorical_crossentropy' , \n",
        "              metrics   = ['accuracy'] ) "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM8QYYUuc-ib",
        "outputId": "25be6fa9-c529-4e15-af8b-e8360f641f6d"
      },
      "source": [
        "history = model.fit(x_train, y_train , batch_size = 128 , epochs = 300 , validation_data=(x_val, y_val) )"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 2.2039 - accuracy: 0.3509 - val_loss: 2.1891 - val_accuracy: 0.3849\n",
            "Epoch 2/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 2.1725 - accuracy: 0.3888 - val_loss: 2.1521 - val_accuracy: 0.3787\n",
            "Epoch 3/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 2.1293 - accuracy: 0.4067 - val_loss: 2.1015 - val_accuracy: 0.4068\n",
            "Epoch 4/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 2.0710 - accuracy: 0.4149 - val_loss: 2.0352 - val_accuracy: 0.4147\n",
            "Epoch 5/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.9979 - accuracy: 0.4226 - val_loss: 1.9557 - val_accuracy: 0.4292\n",
            "Epoch 6/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.9141 - accuracy: 0.4303 - val_loss: 1.8697 - val_accuracy: 0.4582\n",
            "Epoch 7/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.8261 - accuracy: 0.4536 - val_loss: 1.7819 - val_accuracy: 0.4707\n",
            "Epoch 8/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.7376 - accuracy: 0.4732 - val_loss: 1.6946 - val_accuracy: 0.5001\n",
            "Epoch 9/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.6505 - accuracy: 0.5088 - val_loss: 1.6098 - val_accuracy: 0.5202\n",
            "Epoch 10/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.5660 - accuracy: 0.5360 - val_loss: 1.5278 - val_accuracy: 0.5627\n",
            "Epoch 11/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.4847 - accuracy: 0.5705 - val_loss: 1.4500 - val_accuracy: 0.5873\n",
            "Epoch 12/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.4078 - accuracy: 0.5992 - val_loss: 1.3757 - val_accuracy: 0.6142\n",
            "Epoch 13/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.3348 - accuracy: 0.6245 - val_loss: 1.3064 - val_accuracy: 0.6433\n",
            "Epoch 14/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 1.2662 - accuracy: 0.6525 - val_loss: 1.2413 - val_accuracy: 0.6566\n",
            "Epoch 15/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.2013 - accuracy: 0.6699 - val_loss: 1.1789 - val_accuracy: 0.6816\n",
            "Epoch 16/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.1387 - accuracy: 0.6871 - val_loss: 1.1184 - val_accuracy: 0.6985\n",
            "Epoch 17/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.0786 - accuracy: 0.7065 - val_loss: 1.0618 - val_accuracy: 0.7092\n",
            "Epoch 18/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 1.0212 - accuracy: 0.7131 - val_loss: 1.0074 - val_accuracy: 0.7166\n",
            "Epoch 19/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.9663 - accuracy: 0.7237 - val_loss: 0.9588 - val_accuracy: 0.7188\n",
            "Epoch 20/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.9171 - accuracy: 0.7294 - val_loss: 0.9087 - val_accuracy: 0.7303\n",
            "Epoch 21/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.8702 - accuracy: 0.7388 - val_loss: 0.8674 - val_accuracy: 0.7359\n",
            "Epoch 22/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.8294 - accuracy: 0.7426 - val_loss: 0.8308 - val_accuracy: 0.7479\n",
            "Epoch 23/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.7933 - accuracy: 0.7526 - val_loss: 0.8031 - val_accuracy: 0.7483\n",
            "Epoch 24/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.7623 - accuracy: 0.7534 - val_loss: 0.7694 - val_accuracy: 0.7616\n",
            "Epoch 25/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.7332 - accuracy: 0.7673 - val_loss: 0.7450 - val_accuracy: 0.7611\n",
            "Epoch 26/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.7064 - accuracy: 0.7714 - val_loss: 0.7211 - val_accuracy: 0.7772\n",
            "Epoch 27/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.7827 - val_loss: 0.6996 - val_accuracy: 0.7832\n",
            "Epoch 28/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.7915 - val_loss: 0.6815 - val_accuracy: 0.7873\n",
            "Epoch 29/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.7971 - val_loss: 0.6633 - val_accuracy: 0.8079\n",
            "Epoch 30/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.8130 - val_loss: 0.6447 - val_accuracy: 0.8132\n",
            "Epoch 31/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.6052 - accuracy: 0.8228 - val_loss: 0.6337 - val_accuracy: 0.8128\n",
            "Epoch 32/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.5892 - accuracy: 0.8352 - val_loss: 0.6133 - val_accuracy: 0.8291\n",
            "Epoch 33/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.5713 - accuracy: 0.8454 - val_loss: 0.6004 - val_accuracy: 0.8423\n",
            "Epoch 34/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.5554 - accuracy: 0.8567 - val_loss: 0.5836 - val_accuracy: 0.8529\n",
            "Epoch 35/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.5391 - accuracy: 0.8654 - val_loss: 0.5678 - val_accuracy: 0.8590\n",
            "Epoch 36/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.5216 - accuracy: 0.8756 - val_loss: 0.5526 - val_accuracy: 0.8721\n",
            "Epoch 37/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.5054 - accuracy: 0.8839 - val_loss: 0.5364 - val_accuracy: 0.8734\n",
            "Epoch 38/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.4879 - accuracy: 0.8900 - val_loss: 0.5224 - val_accuracy: 0.8828\n",
            "Epoch 39/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.4710 - accuracy: 0.8967 - val_loss: 0.5043 - val_accuracy: 0.8897\n",
            "Epoch 40/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.4540 - accuracy: 0.9016 - val_loss: 0.4956 - val_accuracy: 0.8929\n",
            "Epoch 41/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.4380 - accuracy: 0.9075 - val_loss: 0.4731 - val_accuracy: 0.8966\n",
            "Epoch 42/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.4222 - accuracy: 0.9118 - val_loss: 0.4613 - val_accuracy: 0.9008\n",
            "Epoch 43/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.4053 - accuracy: 0.9155 - val_loss: 0.4470 - val_accuracy: 0.9022\n",
            "Epoch 44/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.3909 - accuracy: 0.9197 - val_loss: 0.4317 - val_accuracy: 0.9068\n",
            "Epoch 45/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.3750 - accuracy: 0.9231 - val_loss: 0.4205 - val_accuracy: 0.9086\n",
            "Epoch 46/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.3627 - accuracy: 0.9237 - val_loss: 0.4055 - val_accuracy: 0.9089\n",
            "Epoch 47/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.3478 - accuracy: 0.9283 - val_loss: 0.3930 - val_accuracy: 0.9142\n",
            "Epoch 48/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.3352 - accuracy: 0.9316 - val_loss: 0.3814 - val_accuracy: 0.9162\n",
            "Epoch 49/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.3225 - accuracy: 0.9334 - val_loss: 0.3718 - val_accuracy: 0.9161\n",
            "Epoch 50/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.3118 - accuracy: 0.9356 - val_loss: 0.3623 - val_accuracy: 0.9172\n",
            "Epoch 51/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.3000 - accuracy: 0.9381 - val_loss: 0.3510 - val_accuracy: 0.9216\n",
            "Epoch 52/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2888 - accuracy: 0.9406 - val_loss: 0.3445 - val_accuracy: 0.9220\n",
            "Epoch 53/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2808 - accuracy: 0.9429 - val_loss: 0.3375 - val_accuracy: 0.9228\n",
            "Epoch 54/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2728 - accuracy: 0.9433 - val_loss: 0.3289 - val_accuracy: 0.9237\n",
            "Epoch 55/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2642 - accuracy: 0.9455 - val_loss: 0.3232 - val_accuracy: 0.9263\n",
            "Epoch 56/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2556 - accuracy: 0.9473 - val_loss: 0.3166 - val_accuracy: 0.9262\n",
            "Epoch 57/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2461 - accuracy: 0.9491 - val_loss: 0.3067 - val_accuracy: 0.9276\n",
            "Epoch 58/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2359 - accuracy: 0.9511 - val_loss: 0.3012 - val_accuracy: 0.9289\n",
            "Epoch 59/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2319 - accuracy: 0.9517 - val_loss: 0.2976 - val_accuracy: 0.9294\n",
            "Epoch 60/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2246 - accuracy: 0.9542 - val_loss: 0.2972 - val_accuracy: 0.9291\n",
            "Epoch 61/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2208 - accuracy: 0.9542 - val_loss: 0.2859 - val_accuracy: 0.9320\n",
            "Epoch 62/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2132 - accuracy: 0.9568 - val_loss: 0.2822 - val_accuracy: 0.9320\n",
            "Epoch 63/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.2079 - accuracy: 0.9574 - val_loss: 0.2828 - val_accuracy: 0.9326\n",
            "Epoch 64/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.2051 - accuracy: 0.9578 - val_loss: 0.2754 - val_accuracy: 0.9337\n",
            "Epoch 65/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1973 - accuracy: 0.9598 - val_loss: 0.2709 - val_accuracy: 0.9337\n",
            "Epoch 66/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1912 - accuracy: 0.9607 - val_loss: 0.2679 - val_accuracy: 0.9336\n",
            "Epoch 67/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1869 - accuracy: 0.9615 - val_loss: 0.2640 - val_accuracy: 0.9352\n",
            "Epoch 68/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1808 - accuracy: 0.9626 - val_loss: 0.2627 - val_accuracy: 0.9346\n",
            "Epoch 69/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1781 - accuracy: 0.9633 - val_loss: 0.2643 - val_accuracy: 0.9324\n",
            "Epoch 70/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1751 - accuracy: 0.9636 - val_loss: 0.2575 - val_accuracy: 0.9360\n",
            "Epoch 71/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1695 - accuracy: 0.9650 - val_loss: 0.2519 - val_accuracy: 0.9368\n",
            "Epoch 72/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1685 - accuracy: 0.9646 - val_loss: 0.2595 - val_accuracy: 0.9346\n",
            "Epoch 73/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1631 - accuracy: 0.9663 - val_loss: 0.2458 - val_accuracy: 0.9383\n",
            "Epoch 74/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1593 - accuracy: 0.9676 - val_loss: 0.2447 - val_accuracy: 0.9389\n",
            "Epoch 75/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1544 - accuracy: 0.9688 - val_loss: 0.2426 - val_accuracy: 0.9387\n",
            "Epoch 76/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.1514 - accuracy: 0.9687 - val_loss: 0.2370 - val_accuracy: 0.9407\n",
            "Epoch 77/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1464 - accuracy: 0.9705 - val_loss: 0.2372 - val_accuracy: 0.9405\n",
            "Epoch 78/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1412 - accuracy: 0.9720 - val_loss: 0.2336 - val_accuracy: 0.9417\n",
            "Epoch 79/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1374 - accuracy: 0.9737 - val_loss: 0.2319 - val_accuracy: 0.9412\n",
            "Epoch 80/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1351 - accuracy: 0.9725 - val_loss: 0.2376 - val_accuracy: 0.9390\n",
            "Epoch 81/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1346 - accuracy: 0.9729 - val_loss: 0.2312 - val_accuracy: 0.9409\n",
            "Epoch 82/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1301 - accuracy: 0.9747 - val_loss: 0.2279 - val_accuracy: 0.9403\n",
            "Epoch 83/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1287 - accuracy: 0.9742 - val_loss: 0.2279 - val_accuracy: 0.9406\n",
            "Epoch 84/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1251 - accuracy: 0.9753 - val_loss: 0.2276 - val_accuracy: 0.9401\n",
            "Epoch 85/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1218 - accuracy: 0.9764 - val_loss: 0.2249 - val_accuracy: 0.9423\n",
            "Epoch 86/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1198 - accuracy: 0.9765 - val_loss: 0.2232 - val_accuracy: 0.9426\n",
            "Epoch 87/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1188 - accuracy: 0.9774 - val_loss: 0.2257 - val_accuracy: 0.9428\n",
            "Epoch 88/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1187 - accuracy: 0.9756 - val_loss: 0.2258 - val_accuracy: 0.9411\n",
            "Epoch 89/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1150 - accuracy: 0.9773 - val_loss: 0.2215 - val_accuracy: 0.9433\n",
            "Epoch 90/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1142 - accuracy: 0.9771 - val_loss: 0.2186 - val_accuracy: 0.9427\n",
            "Epoch 91/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1095 - accuracy: 0.9789 - val_loss: 0.2167 - val_accuracy: 0.9428\n",
            "Epoch 92/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1072 - accuracy: 0.9791 - val_loss: 0.2173 - val_accuracy: 0.9433\n",
            "Epoch 93/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.1054 - accuracy: 0.9795 - val_loss: 0.2152 - val_accuracy: 0.9449\n",
            "Epoch 94/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.1026 - accuracy: 0.9803 - val_loss: 0.2133 - val_accuracy: 0.9437\n",
            "Epoch 95/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0994 - accuracy: 0.9812 - val_loss: 0.2121 - val_accuracy: 0.9449\n",
            "Epoch 96/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0968 - accuracy: 0.9814 - val_loss: 0.2116 - val_accuracy: 0.9435\n",
            "Epoch 97/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0936 - accuracy: 0.9825 - val_loss: 0.2117 - val_accuracy: 0.9448\n",
            "Epoch 98/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0911 - accuracy: 0.9829 - val_loss: 0.2085 - val_accuracy: 0.9453\n",
            "Epoch 99/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0874 - accuracy: 0.9843 - val_loss: 0.2152 - val_accuracy: 0.9433\n",
            "Epoch 100/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0891 - accuracy: 0.9830 - val_loss: 0.2101 - val_accuracy: 0.9432\n",
            "Epoch 101/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0855 - accuracy: 0.9842 - val_loss: 0.2066 - val_accuracy: 0.9460\n",
            "Epoch 102/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0812 - accuracy: 0.9860 - val_loss: 0.2061 - val_accuracy: 0.9456\n",
            "Epoch 103/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0799 - accuracy: 0.9862 - val_loss: 0.2092 - val_accuracy: 0.9442\n",
            "Epoch 104/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0823 - accuracy: 0.9854 - val_loss: 0.2062 - val_accuracy: 0.9457\n",
            "Epoch 105/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0782 - accuracy: 0.9867 - val_loss: 0.2061 - val_accuracy: 0.9448\n",
            "Epoch 106/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0746 - accuracy: 0.9876 - val_loss: 0.2091 - val_accuracy: 0.9441\n",
            "Epoch 107/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0738 - accuracy: 0.9877 - val_loss: 0.2055 - val_accuracy: 0.9456\n",
            "Epoch 108/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0732 - accuracy: 0.9873 - val_loss: 0.2034 - val_accuracy: 0.9457\n",
            "Epoch 109/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0697 - accuracy: 0.9887 - val_loss: 0.2020 - val_accuracy: 0.9458\n",
            "Epoch 110/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0677 - accuracy: 0.9897 - val_loss: 0.2016 - val_accuracy: 0.9457\n",
            "Epoch 111/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0670 - accuracy: 0.9894 - val_loss: 0.2039 - val_accuracy: 0.9455\n",
            "Epoch 112/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0656 - accuracy: 0.9896 - val_loss: 0.2042 - val_accuracy: 0.9449\n",
            "Epoch 113/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0632 - accuracy: 0.9904 - val_loss: 0.1980 - val_accuracy: 0.9476\n",
            "Epoch 114/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0597 - accuracy: 0.9916 - val_loss: 0.1989 - val_accuracy: 0.9480\n",
            "Epoch 115/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0581 - accuracy: 0.9915 - val_loss: 0.1998 - val_accuracy: 0.9462\n",
            "Epoch 116/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0559 - accuracy: 0.9925 - val_loss: 0.1991 - val_accuracy: 0.9459\n",
            "Epoch 117/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0551 - accuracy: 0.9923 - val_loss: 0.2009 - val_accuracy: 0.9459\n",
            "Epoch 118/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0540 - accuracy: 0.9926 - val_loss: 0.2009 - val_accuracy: 0.9463\n",
            "Epoch 119/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0533 - accuracy: 0.9928 - val_loss: 0.2032 - val_accuracy: 0.9445\n",
            "Epoch 120/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0520 - accuracy: 0.9929 - val_loss: 0.2043 - val_accuracy: 0.9456\n",
            "Epoch 121/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0508 - accuracy: 0.9928 - val_loss: 0.2014 - val_accuracy: 0.9453\n",
            "Epoch 122/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0485 - accuracy: 0.9935 - val_loss: 0.1994 - val_accuracy: 0.9456\n",
            "Epoch 123/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0461 - accuracy: 0.9945 - val_loss: 0.2051 - val_accuracy: 0.9449\n",
            "Epoch 124/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0473 - accuracy: 0.9938 - val_loss: 0.2010 - val_accuracy: 0.9454\n",
            "Epoch 125/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0455 - accuracy: 0.9941 - val_loss: 0.2015 - val_accuracy: 0.9455\n",
            "Epoch 126/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0434 - accuracy: 0.9947 - val_loss: 0.1996 - val_accuracy: 0.9467\n",
            "Epoch 127/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0411 - accuracy: 0.9950 - val_loss: 0.2015 - val_accuracy: 0.9463\n",
            "Epoch 128/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0392 - accuracy: 0.9956 - val_loss: 0.1999 - val_accuracy: 0.9458\n",
            "Epoch 129/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0381 - accuracy: 0.9957 - val_loss: 0.1998 - val_accuracy: 0.9448\n",
            "Epoch 130/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0373 - accuracy: 0.9958 - val_loss: 0.2006 - val_accuracy: 0.9457\n",
            "Epoch 131/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0359 - accuracy: 0.9959 - val_loss: 0.2003 - val_accuracy: 0.9459\n",
            "Epoch 132/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0347 - accuracy: 0.9961 - val_loss: 0.1995 - val_accuracy: 0.9462\n",
            "Epoch 133/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0338 - accuracy: 0.9963 - val_loss: 0.1989 - val_accuracy: 0.9469\n",
            "Epoch 134/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0325 - accuracy: 0.9964 - val_loss: 0.1995 - val_accuracy: 0.9465\n",
            "Epoch 135/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0323 - accuracy: 0.9964 - val_loss: 0.1985 - val_accuracy: 0.9463\n",
            "Epoch 136/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0315 - accuracy: 0.9965 - val_loss: 0.1992 - val_accuracy: 0.9463\n",
            "Epoch 137/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0309 - accuracy: 0.9965 - val_loss: 0.1992 - val_accuracy: 0.9466\n",
            "Epoch 138/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0301 - accuracy: 0.9966 - val_loss: 0.1991 - val_accuracy: 0.9464\n",
            "Epoch 139/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0295 - accuracy: 0.9967 - val_loss: 0.2010 - val_accuracy: 0.9461\n",
            "Epoch 140/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0289 - accuracy: 0.9968 - val_loss: 0.1995 - val_accuracy: 0.9468\n",
            "Epoch 141/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0283 - accuracy: 0.9968 - val_loss: 0.1999 - val_accuracy: 0.9474\n",
            "Epoch 142/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0279 - accuracy: 0.9968 - val_loss: 0.2008 - val_accuracy: 0.9468\n",
            "Epoch 143/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0274 - accuracy: 0.9968 - val_loss: 0.1992 - val_accuracy: 0.9471\n",
            "Epoch 144/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0270 - accuracy: 0.9968 - val_loss: 0.1994 - val_accuracy: 0.9475\n",
            "Epoch 145/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0266 - accuracy: 0.9969 - val_loss: 0.2002 - val_accuracy: 0.9464\n",
            "Epoch 146/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0262 - accuracy: 0.9969 - val_loss: 0.1995 - val_accuracy: 0.9473\n",
            "Epoch 147/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0260 - accuracy: 0.9969 - val_loss: 0.2003 - val_accuracy: 0.9465\n",
            "Epoch 148/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0256 - accuracy: 0.9969 - val_loss: 0.2006 - val_accuracy: 0.9463\n",
            "Epoch 149/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0253 - accuracy: 0.9971 - val_loss: 0.2006 - val_accuracy: 0.9464\n",
            "Epoch 150/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0251 - accuracy: 0.9970 - val_loss: 0.2002 - val_accuracy: 0.9468\n",
            "Epoch 151/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0247 - accuracy: 0.9971 - val_loss: 0.2009 - val_accuracy: 0.9466\n",
            "Epoch 152/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0245 - accuracy: 0.9970 - val_loss: 0.2003 - val_accuracy: 0.9463\n",
            "Epoch 153/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0242 - accuracy: 0.9971 - val_loss: 0.2008 - val_accuracy: 0.9467\n",
            "Epoch 154/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9971 - val_loss: 0.2005 - val_accuracy: 0.9468\n",
            "Epoch 155/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0237 - accuracy: 0.9971 - val_loss: 0.2008 - val_accuracy: 0.9462\n",
            "Epoch 156/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0234 - accuracy: 0.9972 - val_loss: 0.2006 - val_accuracy: 0.9468\n",
            "Epoch 157/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0232 - accuracy: 0.9972 - val_loss: 0.2010 - val_accuracy: 0.9459\n",
            "Epoch 158/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0230 - accuracy: 0.9972 - val_loss: 0.2011 - val_accuracy: 0.9466\n",
            "Epoch 159/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0228 - accuracy: 0.9972 - val_loss: 0.2010 - val_accuracy: 0.9461\n",
            "Epoch 160/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0226 - accuracy: 0.9972 - val_loss: 0.2006 - val_accuracy: 0.9466\n",
            "Epoch 161/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0224 - accuracy: 0.9972 - val_loss: 0.2015 - val_accuracy: 0.9462\n",
            "Epoch 162/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0222 - accuracy: 0.9972 - val_loss: 0.2016 - val_accuracy: 0.9459\n",
            "Epoch 163/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0220 - accuracy: 0.9972 - val_loss: 0.2015 - val_accuracy: 0.9462\n",
            "Epoch 164/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0218 - accuracy: 0.9972 - val_loss: 0.2016 - val_accuracy: 0.9458\n",
            "Epoch 165/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0216 - accuracy: 0.9973 - val_loss: 0.2020 - val_accuracy: 0.9462\n",
            "Epoch 166/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0214 - accuracy: 0.9973 - val_loss: 0.2019 - val_accuracy: 0.9459\n",
            "Epoch 167/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0212 - accuracy: 0.9973 - val_loss: 0.2018 - val_accuracy: 0.9463\n",
            "Epoch 168/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0211 - accuracy: 0.9973 - val_loss: 0.2016 - val_accuracy: 0.9468\n",
            "Epoch 169/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9973 - val_loss: 0.2016 - val_accuracy: 0.9462\n",
            "Epoch 170/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0207 - accuracy: 0.9974 - val_loss: 0.2019 - val_accuracy: 0.9464\n",
            "Epoch 171/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0206 - accuracy: 0.9974 - val_loss: 0.2025 - val_accuracy: 0.9461\n",
            "Epoch 172/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0204 - accuracy: 0.9974 - val_loss: 0.2023 - val_accuracy: 0.9462\n",
            "Epoch 173/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0202 - accuracy: 0.9974 - val_loss: 0.2021 - val_accuracy: 0.9462\n",
            "Epoch 174/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0201 - accuracy: 0.9974 - val_loss: 0.2025 - val_accuracy: 0.9463\n",
            "Epoch 175/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0199 - accuracy: 0.9974 - val_loss: 0.2024 - val_accuracy: 0.9467\n",
            "Epoch 176/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0198 - accuracy: 0.9974 - val_loss: 0.2024 - val_accuracy: 0.9463\n",
            "Epoch 177/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0196 - accuracy: 0.9974 - val_loss: 0.2022 - val_accuracy: 0.9459\n",
            "Epoch 178/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9975 - val_loss: 0.2029 - val_accuracy: 0.9461\n",
            "Epoch 179/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0194 - accuracy: 0.9974 - val_loss: 0.2023 - val_accuracy: 0.9458\n",
            "Epoch 180/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9975 - val_loss: 0.2026 - val_accuracy: 0.9461\n",
            "Epoch 181/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0191 - accuracy: 0.9975 - val_loss: 0.2030 - val_accuracy: 0.9462\n",
            "Epoch 182/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0190 - accuracy: 0.9975 - val_loss: 0.2030 - val_accuracy: 0.9460\n",
            "Epoch 183/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9975 - val_loss: 0.2026 - val_accuracy: 0.9460\n",
            "Epoch 184/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9975 - val_loss: 0.2036 - val_accuracy: 0.9457\n",
            "Epoch 185/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0186 - accuracy: 0.9975 - val_loss: 0.2031 - val_accuracy: 0.9462\n",
            "Epoch 186/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0185 - accuracy: 0.9976 - val_loss: 0.2032 - val_accuracy: 0.9464\n",
            "Epoch 187/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0183 - accuracy: 0.9976 - val_loss: 0.2032 - val_accuracy: 0.9463\n",
            "Epoch 188/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0182 - accuracy: 0.9976 - val_loss: 0.2033 - val_accuracy: 0.9465\n",
            "Epoch 189/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0181 - accuracy: 0.9976 - val_loss: 0.2032 - val_accuracy: 0.9464\n",
            "Epoch 190/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0179 - accuracy: 0.9976 - val_loss: 0.2034 - val_accuracy: 0.9468\n",
            "Epoch 191/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0178 - accuracy: 0.9976 - val_loss: 0.2035 - val_accuracy: 0.9468\n",
            "Epoch 192/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0177 - accuracy: 0.9976 - val_loss: 0.2035 - val_accuracy: 0.9463\n",
            "Epoch 193/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9976 - val_loss: 0.2035 - val_accuracy: 0.9464\n",
            "Epoch 194/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9977 - val_loss: 0.2038 - val_accuracy: 0.9461\n",
            "Epoch 195/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0174 - accuracy: 0.9977 - val_loss: 0.2040 - val_accuracy: 0.9459\n",
            "Epoch 196/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0173 - accuracy: 0.9977 - val_loss: 0.2038 - val_accuracy: 0.9462\n",
            "Epoch 197/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0172 - accuracy: 0.9977 - val_loss: 0.2042 - val_accuracy: 0.9462\n",
            "Epoch 198/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0171 - accuracy: 0.9977 - val_loss: 0.2043 - val_accuracy: 0.9462\n",
            "Epoch 199/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0170 - accuracy: 0.9977 - val_loss: 0.2045 - val_accuracy: 0.9463\n",
            "Epoch 200/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0169 - accuracy: 0.9977 - val_loss: 0.2043 - val_accuracy: 0.9464\n",
            "Epoch 201/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0167 - accuracy: 0.9978 - val_loss: 0.2043 - val_accuracy: 0.9462\n",
            "Epoch 202/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0166 - accuracy: 0.9978 - val_loss: 0.2045 - val_accuracy: 0.9462\n",
            "Epoch 203/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0165 - accuracy: 0.9978 - val_loss: 0.2047 - val_accuracy: 0.9464\n",
            "Epoch 204/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0164 - accuracy: 0.9978 - val_loss: 0.2048 - val_accuracy: 0.9462\n",
            "Epoch 205/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0164 - accuracy: 0.9978 - val_loss: 0.2049 - val_accuracy: 0.9465\n",
            "Epoch 206/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0163 - accuracy: 0.9978 - val_loss: 0.2052 - val_accuracy: 0.9458\n",
            "Epoch 207/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0162 - accuracy: 0.9978 - val_loss: 0.2052 - val_accuracy: 0.9461\n",
            "Epoch 208/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0161 - accuracy: 0.9978 - val_loss: 0.2052 - val_accuracy: 0.9461\n",
            "Epoch 209/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0160 - accuracy: 0.9979 - val_loss: 0.2051 - val_accuracy: 0.9463\n",
            "Epoch 210/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0159 - accuracy: 0.9979 - val_loss: 0.2053 - val_accuracy: 0.9461\n",
            "Epoch 211/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0158 - accuracy: 0.9979 - val_loss: 0.2053 - val_accuracy: 0.9463\n",
            "Epoch 212/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0157 - accuracy: 0.9979 - val_loss: 0.2053 - val_accuracy: 0.9463\n",
            "Epoch 213/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0156 - accuracy: 0.9979 - val_loss: 0.2055 - val_accuracy: 0.9464\n",
            "Epoch 214/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0155 - accuracy: 0.9979 - val_loss: 0.2055 - val_accuracy: 0.9464\n",
            "Epoch 215/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0155 - accuracy: 0.9979 - val_loss: 0.2058 - val_accuracy: 0.9460\n",
            "Epoch 216/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0154 - accuracy: 0.9979 - val_loss: 0.2057 - val_accuracy: 0.9461\n",
            "Epoch 217/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0153 - accuracy: 0.9979 - val_loss: 0.2061 - val_accuracy: 0.9456\n",
            "Epoch 218/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0152 - accuracy: 0.9979 - val_loss: 0.2065 - val_accuracy: 0.9456\n",
            "Epoch 219/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0151 - accuracy: 0.9979 - val_loss: 0.2062 - val_accuracy: 0.9461\n",
            "Epoch 220/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0151 - accuracy: 0.9979 - val_loss: 0.2068 - val_accuracy: 0.9457\n",
            "Epoch 221/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0150 - accuracy: 0.9979 - val_loss: 0.2067 - val_accuracy: 0.9454\n",
            "Epoch 222/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0149 - accuracy: 0.9980 - val_loss: 0.2067 - val_accuracy: 0.9457\n",
            "Epoch 223/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0148 - accuracy: 0.9980 - val_loss: 0.2066 - val_accuracy: 0.9463\n",
            "Epoch 224/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0147 - accuracy: 0.9980 - val_loss: 0.2066 - val_accuracy: 0.9458\n",
            "Epoch 225/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0147 - accuracy: 0.9979 - val_loss: 0.2071 - val_accuracy: 0.9456\n",
            "Epoch 226/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0146 - accuracy: 0.9980 - val_loss: 0.2068 - val_accuracy: 0.9459\n",
            "Epoch 227/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0145 - accuracy: 0.9980 - val_loss: 0.2072 - val_accuracy: 0.9461\n",
            "Epoch 228/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0144 - accuracy: 0.9980 - val_loss: 0.2072 - val_accuracy: 0.9461\n",
            "Epoch 229/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0144 - accuracy: 0.9980 - val_loss: 0.2072 - val_accuracy: 0.9461\n",
            "Epoch 230/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0143 - accuracy: 0.9980 - val_loss: 0.2074 - val_accuracy: 0.9456\n",
            "Epoch 231/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0142 - accuracy: 0.9980 - val_loss: 0.2074 - val_accuracy: 0.9461\n",
            "Epoch 232/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0142 - accuracy: 0.9980 - val_loss: 0.2074 - val_accuracy: 0.9459\n",
            "Epoch 233/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0141 - accuracy: 0.9980 - val_loss: 0.2077 - val_accuracy: 0.9458\n",
            "Epoch 234/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0140 - accuracy: 0.9980 - val_loss: 0.2078 - val_accuracy: 0.9455\n",
            "Epoch 235/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0140 - accuracy: 0.9980 - val_loss: 0.2086 - val_accuracy: 0.9456\n",
            "Epoch 236/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.2081 - val_accuracy: 0.9460\n",
            "Epoch 237/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.2084 - val_accuracy: 0.9457\n",
            "Epoch 238/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0138 - accuracy: 0.9980 - val_loss: 0.2081 - val_accuracy: 0.9461\n",
            "Epoch 239/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0137 - accuracy: 0.9981 - val_loss: 0.2088 - val_accuracy: 0.9456\n",
            "Epoch 240/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0137 - accuracy: 0.9981 - val_loss: 0.2084 - val_accuracy: 0.9457\n",
            "Epoch 241/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0136 - accuracy: 0.9981 - val_loss: 0.2085 - val_accuracy: 0.9461\n",
            "Epoch 242/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0135 - accuracy: 0.9980 - val_loss: 0.2083 - val_accuracy: 0.9459\n",
            "Epoch 243/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0134 - accuracy: 0.9981 - val_loss: 0.2086 - val_accuracy: 0.9456\n",
            "Epoch 244/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0134 - accuracy: 0.9981 - val_loss: 0.2087 - val_accuracy: 0.9461\n",
            "Epoch 245/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0133 - accuracy: 0.9981 - val_loss: 0.2083 - val_accuracy: 0.9461\n",
            "Epoch 246/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0133 - accuracy: 0.9981 - val_loss: 0.2089 - val_accuracy: 0.9456\n",
            "Epoch 247/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0132 - accuracy: 0.9982 - val_loss: 0.2091 - val_accuracy: 0.9461\n",
            "Epoch 248/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0131 - accuracy: 0.9982 - val_loss: 0.2092 - val_accuracy: 0.9460\n",
            "Epoch 249/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0130 - accuracy: 0.9982 - val_loss: 0.2097 - val_accuracy: 0.9461\n",
            "Epoch 250/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0130 - accuracy: 0.9982 - val_loss: 0.2095 - val_accuracy: 0.9457\n",
            "Epoch 251/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0129 - accuracy: 0.9982 - val_loss: 0.2095 - val_accuracy: 0.9461\n",
            "Epoch 252/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0129 - accuracy: 0.9982 - val_loss: 0.2094 - val_accuracy: 0.9459\n",
            "Epoch 253/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.2097 - val_accuracy: 0.9457\n",
            "Epoch 254/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.2100 - val_accuracy: 0.9462\n",
            "Epoch 255/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0127 - accuracy: 0.9982 - val_loss: 0.2101 - val_accuracy: 0.9462\n",
            "Epoch 256/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.2101 - val_accuracy: 0.9457\n",
            "Epoch 257/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0126 - accuracy: 0.9982 - val_loss: 0.2101 - val_accuracy: 0.9464\n",
            "Epoch 258/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.2105 - val_accuracy: 0.9462\n",
            "Epoch 259/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.2105 - val_accuracy: 0.9461\n",
            "Epoch 260/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.2105 - val_accuracy: 0.9459\n",
            "Epoch 261/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0123 - accuracy: 0.9984 - val_loss: 0.2108 - val_accuracy: 0.9461\n",
            "Epoch 262/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.2110 - val_accuracy: 0.9459\n",
            "Epoch 263/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0122 - accuracy: 0.9984 - val_loss: 0.2106 - val_accuracy: 0.9461\n",
            "Epoch 264/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0122 - accuracy: 0.9984 - val_loss: 0.2110 - val_accuracy: 0.9458\n",
            "Epoch 265/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.2111 - val_accuracy: 0.9459\n",
            "Epoch 266/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.2111 - val_accuracy: 0.9459\n",
            "Epoch 267/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.2110 - val_accuracy: 0.9460\n",
            "Epoch 268/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0120 - accuracy: 0.9984 - val_loss: 0.2112 - val_accuracy: 0.9456\n",
            "Epoch 269/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0119 - accuracy: 0.9984 - val_loss: 0.2112 - val_accuracy: 0.9457\n",
            "Epoch 270/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0119 - accuracy: 0.9984 - val_loss: 0.2116 - val_accuracy: 0.9456\n",
            "Epoch 271/300\n",
            "329/329 [==============================] - 1s 3ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 0.2116 - val_accuracy: 0.9459\n",
            "Epoch 272/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 0.2116 - val_accuracy: 0.9457\n",
            "Epoch 273/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0117 - accuracy: 0.9985 - val_loss: 0.2117 - val_accuracy: 0.9457\n",
            "Epoch 274/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0117 - accuracy: 0.9984 - val_loss: 0.2118 - val_accuracy: 0.9454\n",
            "Epoch 275/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0117 - accuracy: 0.9984 - val_loss: 0.2122 - val_accuracy: 0.9458\n",
            "Epoch 276/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.2116 - val_accuracy: 0.9458\n",
            "Epoch 277/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0116 - accuracy: 0.9984 - val_loss: 0.2116 - val_accuracy: 0.9454\n",
            "Epoch 278/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.2122 - val_accuracy: 0.9457\n",
            "Epoch 279/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.2126 - val_accuracy: 0.9454\n",
            "Epoch 280/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.2126 - val_accuracy: 0.9452\n",
            "Epoch 281/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0113 - accuracy: 0.9985 - val_loss: 0.2124 - val_accuracy: 0.9456\n",
            "Epoch 282/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0113 - accuracy: 0.9985 - val_loss: 0.2128 - val_accuracy: 0.9456\n",
            "Epoch 283/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.2128 - val_accuracy: 0.9456\n",
            "Epoch 284/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.2130 - val_accuracy: 0.9455\n",
            "Epoch 285/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.2129 - val_accuracy: 0.9457\n",
            "Epoch 286/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.2131 - val_accuracy: 0.9456\n",
            "Epoch 287/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.2133 - val_accuracy: 0.9453\n",
            "Epoch 288/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.2136 - val_accuracy: 0.9456\n",
            "Epoch 289/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.2135 - val_accuracy: 0.9456\n",
            "Epoch 290/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.2135 - val_accuracy: 0.9455\n",
            "Epoch 291/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.2136 - val_accuracy: 0.9454\n",
            "Epoch 292/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 0.2135 - val_accuracy: 0.9456\n",
            "Epoch 293/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 0.2137 - val_accuracy: 0.9458\n",
            "Epoch 294/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 0.2136 - val_accuracy: 0.9456\n",
            "Epoch 295/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0107 - accuracy: 0.9986 - val_loss: 0.2137 - val_accuracy: 0.9456\n",
            "Epoch 296/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0107 - accuracy: 0.9986 - val_loss: 0.2140 - val_accuracy: 0.9456\n",
            "Epoch 297/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0106 - accuracy: 0.9986 - val_loss: 0.2143 - val_accuracy: 0.9456\n",
            "Epoch 298/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0106 - accuracy: 0.9986 - val_loss: 0.2144 - val_accuracy: 0.9456\n",
            "Epoch 299/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0106 - accuracy: 0.9986 - val_loss: 0.2143 - val_accuracy: 0.9458\n",
            "Epoch 300/300\n",
            "329/329 [==============================] - 1s 4ms/step - loss: 0.0105 - accuracy: 0.9986 - val_loss: 0.2145 - val_accuracy: 0.9454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlXXRjhWdCS3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}